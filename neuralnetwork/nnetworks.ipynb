{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5888854-6c69-4b2f-b224-67ff02afa62c",
   "metadata": {},
   "source": [
    "# Exploring Hyperparameters in Neural Networks with PyTorch\n",
    "\n",
    "- Author: Sebastian Schuetz, swschuetz@outlook.com\n",
    "- Date: Feb 12, 2025\n",
    "\n",
    "**Objective:**  \n",
    "In this assignment, you will explore how different hyperparameters affect the performance of a neural network. We will:\n",
    "\n",
    "1. Create a simple two-class classification problem.\n",
    "2. Build a neural network using PyTorch.\n",
    "3. Experiment with:\n",
    "    - **Learning Rate**: Try different learning rates (e.g., 0.1, 0.01, 0.001).\n",
    "    - **Epochs**: Compare more vs. fewers epochs. \n",
    "    - **Activation Functions**: Compare using ReLU versus Softplus.\n",
    "\n",
    "**Note:** You do not need advanced coding skills to complete this assignment. Follow the instructions and comments carefully.\n",
    "\n",
    "---\n",
    "## Assignment Tasks:\n",
    "\n",
    "1. **Experimentation**: In this workbook, there are four experiments. Conduct them carefully by changing the model parameters as explained in each experiment. After modifying the parameters, execute the code blocks and observe how the model behavior and performance changes.\n",
    "\n",
    "2. **Documentation**: Document the outcomes of your experiments by explaining how changes in a hyperparameter affected model outcomes. Use the assignment template to document your findings.\n",
    "\n",
    "3. **Submission**: Submit your documented findings to the assignment on CANVAS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7501b28c-884f-4341-bee4-a4480858c61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# For creating a synthetic \"moons\" dataset\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "torch.manual_seed(4)\n",
    "np.random.seed(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bf8031-46a5-42d8-9d67-11455e13da60",
   "metadata": {},
   "source": [
    "## Step 1: Create a Synthetic Dataset\n",
    "We will use scikit-learn's `make_moons` to generate a two-class dataset. This dataset will be split into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbb4900-358c-4a6c-b902-a73df0b09588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a \"moons\" dataset with 200 samples\n",
    "X, y = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
    "\n",
    "# Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors and set dtype to float32\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test  = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test  = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Optional: Visualize the dataset\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis', label='Train')\n",
    "plt.title(\"Training Data\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc08058-887a-492f-82f8-41d65f28c273",
   "metadata": {},
   "source": [
    "## Step 2: Define the Neural Network Model\n",
    "We will create a simple network with:\n",
    "- One hidden layer.\n",
    "- A configurable activation function.\n",
    "- Two outputs (for two classes).\n",
    "\n",
    "<div style=\"border: 2px solid red; padding: 10px; color: red;\">\n",
    "<b>Experiment 3:</b> Change the activation function from ReLU to Softplus. Observe how it affects the loss, the decision boundary and test accuracy. \n",
    "<p></p>\n",
    "You can change the activation function by uncommenting the line below the 'Configuration 2' comment in the code below.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccd3dc3-9557-4e10-b0db-1a6487697802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the simple neural network model\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=16, output_dim=2, activation_func=nn.ReLU):\n",
    "        \"\"\"\n",
    "        input_dim: Number of input features\n",
    "        hidden_dim: Number of neurons in the hidden layer\n",
    "        output_dim: Number of output classes\n",
    "        activation_func: Activation function class (e.g., nn.ReLU or nn.Softplus)\n",
    "        \"\"\"\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.act = activation_func()  # Instantiate the activation function\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Configuration 1: Instantiate the network using ReLU as the activation function.\n",
    "net = SimpleNet(activation_func=nn.ReLU)\n",
    "\n",
    "# Configuration 2: Instantiate the network using Softplus as the activation function by uncommenting the \n",
    "# next line (i.e., removing the leading #).\n",
    "\n",
    "#net = SimpleNet(activation_func=nn.Softplus)\n",
    "\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d753e34-263c-45fb-b7a2-051a3b68e426",
   "metadata": {},
   "source": [
    "## Step 3: Define Weight Initialization\n",
    "Weight initialization can affect how fast the network learns.\n",
    "\n",
    "We provide a simple function that you can apply to your model.\n",
    "**Task:**  \n",
    " - Try using Xavier (Glorot) initialization.\n",
    " - (Optionally) Experiment with other initializations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a2bc3c-1925-4180-a205-401c67c27cb9",
   "metadata": {},
   "source": [
    "## Step 4: Train the Network\n",
    "We will now define a training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec632ca-1425-4fc5-86a6-ea8c60d7272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, learning_rate=0.01, epochs=100):\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "        \n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_history.append(loss.item())\n",
    "        \n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "            \n",
    "    return loss_history\n",
    "\n",
    "def reset_weights(m):\n",
    "    if hasattr(m, 'reset_parameters'):\n",
    "        m.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5e8433-34a7-44a8-8114-96719f36e409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf08f7d8-3755-45b7-9c69-aa07bd3687ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57988a10-5c0f-442f-8d16-3b793007e63b",
   "metadata": {},
   "source": [
    "Now we will run the training. The following two experiments apply to the code below:\n",
    "\n",
    "<div style=\"border: 2px solid red; padding: 10px; color: red;\">\n",
    "<b>Experiment 1:</b> Adjust the learning rate. Try different values (e.g., 0.01, 0.10, 0.5, 0.7, 0.9). Observe (and note down) how different learning rates affect training outcomes (i.e., loss after 100 epochs, test accuracy).\n",
    "</div>\n",
    "\n",
    "<p></p>\n",
    "\n",
    "<div style=\"border: 2px solid red; padding: 10px; color: red;\">\n",
    "<b>Experiment 2:</b> Adjust the number of epochs. Try different values (e.g., 10, 100, 200, 1000). Observe (and note down) how different learning rates affect training outcomes (i.e., loss and test accuracy).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34de37a-4a9f-4d28-8168-a57573929e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.apply(reset_weights)\n",
    "\n",
    "loss_history = train_model(net, X_train, y_train, learning_rate=0.7, epochs=100)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(loss_history, label=\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5290b62-b9ae-4d6a-9e6b-a45d6d7f6c99",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the Model and Visualize the Decision Boundary\n",
    "Now, let's evaluate our model on the test set and plot the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80da1f34-d15a-4069-8967-0eea4a930c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, title=\"Decision Boundary\"):\n",
    "    # Create a mesh grid over the data\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    \n",
    "    # Prepare grid points for prediction\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    grid_tensor = torch.tensor(grid, dtype=torch.float32)\n",
    "    \n",
    "    # Get model predictions\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(grid_tensor)\n",
    "        predictions = torch.argmax(outputs, dim=1).numpy()\n",
    "    \n",
    "    # Plot the contour and data points\n",
    "    predictions = predictions.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, predictions, alpha=0.3, cmap='viridis')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='k')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot decision boundary on the training data\n",
    "plot_decision_boundary(net, X_train.numpy(), y_train.numpy(), title=\"Decision Boundary (Train Data)\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = net(X_test)\n",
    "    test_preds = torch.argmax(test_outputs, dim=1)\n",
    "    accuracy = (test_preds == y_test).float().mean()\n",
    "    print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "# Plot decision boundary on the test data\n",
    "plot_decision_boundary(net, X_test.numpy(), y_test.numpy(), title=\"Decision Boundary (Test Data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e65e060-3036-41ee-94b7-886a3459a1c1",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid red; padding: 10px; color: red;\">\n",
    "<b>Experiment 4:</b> Can you find the optimal combination of learning rate, epochs, and activation function that maximizes the models accuracy?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dc5089-eb5b-4e1e-bdb7-feb0de94911e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
